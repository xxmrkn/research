\documentclass[a4paper, oneside, openany, dvipdfmx]{suribt}% 本文が * ページ以下のときに (掲示に注意)
\usepackage{graphicx}
\usepackage{newtxtext,newtxmath}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{slashbox}
\usepackage[unicode, ipaex]{pxchfon}
\usepackage{here}
\usepackage[hang,small,bf]{caption}
\usepackage[subrefformat=parens]{subcaption}
\captionsetup{compatibility=false}
\title{事前学習がVision Transformerに与える影響}
%\titlewidth{}% タイトル幅 (指定するときは単位つきで)
\author{桝田 修慎}
\eauthor{Masachika Masuda}% Copyright 表示で使われる
\studentid{18A1066}
\supervisor{山口裕 助教}% 1 つ引数をとる (役職まで含めて書く)
%\supervisor{指導教員名 役職 \and 指導教員名 役職}% 複数教員の場合，\and でつなげる
\handin{2022}{2}% 提出月. 2 つ (年, 月) 引数をとる
\keywords{Vision Transformer} % 概要の下に表示される

%・データ構成とはなにか、>>> 2章ではどんなことをして、3章ではどんなことをするのか…
%・実験結果の図が多い、付録に入れるべきか >>> 卒論であればメインに入れ込んで良い
%・データ拡張の図は理論のところに入れるべきか、手順のところにいれるべきか >>> 上に同じ
%・サブセクションまで目次に表示させるべきか >>> どちらでも良い
%・図は基本的に自作 >>> 自作、元論文とほとんど同じ場合は引用する
%・図のグリッドは必要か >>> どちらでも良い

\newcommand{\fref}[1]{図\ref{#1}}
\newcommand{\tref}[1]{表\ref{#1}}
\newcommand{\eref}[1]{式\eqref{#1}}

\begin{document}
\maketitle%%%%%%%%%%%%%%%%%%% タイトル %%%%

\frontmatter% ここから前文
\begin{abstract}%%%%%%%%%%%%% 概要 %%%%%%%%
画像認識分野では，畳み込み層を重ねることで良い認識結果をもたらしたVGGをはじめ，残差機構を取り入れ，更に層を深くすることを可能にしたResNetがState-of-the-artを達成し，デファクトスタンダードとなっていた．
しかし，今回のテーマであるVision Transformerは畳み込み層を使っておらず，Attention層を用いて代用している．
また，Vision Transformerでの大規模なデータセットを用いた事前学習は，認識結果に影響し，ImageNet ReaL，CIFAR-100，VTABなどの画像認識のベンチマークでState-of-the-artを達成し，更新した．


\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents%%%%%%%%%%%%% 目次 %%%%%%%%

\mainmatter% ここから本文 %%% 本文 %%%%%%%%
\chapter{序論}
\section{背景}
近年，画像認識分野では，機械翻訳で脚光を浴びることになったTransformer\cite{vaswani2017attention}をコンピュータビジョンに適応させたVision Transformer（以下ViTと称する）が登場した\cite{dosovitskiy2021image}．
ViTは，層を深くし畳み込みを行う畳み込みニューラルネットワーク（以下CNNと称する）とは違い，畳み込み演算をAttention機構を用いて代用しており，特に大規模なデータで事前学習を行なったときの，小・中規模の画像認識ベンチマーク（ImageNet ReaL，CIFAR-100，VTAB，etc．）では，
過去のState-of-the-artのCNNモデルと比べて少ない計算リソースで訓練することができ、更に素晴らしい結果も出している．

本研究では，Vision Transformerが提案された論文
「An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale」を参考にし，事前学習やデータ拡張の有無が，学習及び推論に与える影響を検証した．
\section{本研究の目的}
本研究の目的を以下に示す．
\begin{enumerate}
  \item 一定の条件下での振る舞いを従来のモデル（VGG，ResNet）と比較し，ViTの優れている点・そうではない点を明らかにする．
  \item 事前学習やデータ拡張が各モデルに及ぼす影響を調べる．
\end{enumerate}

\section{深層学習}
深層学習とは，脳の神経回路を模したニューラルネットワークをより深くしたものを指し，入力データから有用な特徴量を自動で抽出する手法である．
\subsection{畳み込みニューラルネットワーク}
畳み込みニューラルネットワーク（CNN）は，入力した画像に対して重み行列（カーネル）を移動させながらスカラ積を求めていく畳み込み層を複数重ねているネットワークのことである．
カーネルは画像全体に同じものを適用するため，CNNは移動させたカーネルと画像のスカラ積による局所性と，画像全体に渡る同一カーネルの重みの使用による移動不変性を持つ．
%移動不変性：局所的なパターンが画像内の物体位置にかかわらず、出力に対して効果的であること．局所性：近傍における局所的な演算処理
\subsection{VGG}
VGGはCNNの一種で，3x3という小さいカーネルを用いており，2014年当時では珍しい16層及び19層の深いネットワークである\cite{simonyan2015deep}．
畳み込みとプーリング，線形層のシンプルなアーキテクチャであるにも関わらず，2014年のImageNetチャレンジのローカリゼーション・クラシフィケーションタスクで1位と2位を収めている．

\subsection{ResNet}
一般的に，CNNでは層を重ねることで，より高次元な特徴を抽出することができるが，層が深くなるにつれて勾配が発散・消失するという問題があった．
しかし，ResNetは，畳み込み層を重ねるだけではなく，前の層，もしくはより浅い層の出力を次の層の入力とする残差機構（スキップ接続）を取り入れることで，より畳み込み層を多く積み重ねながらも，SoTAを達成した\cite{he2015deep}．
ResNetのブロックの一部を\fref{fig:res_arch}に示す．入力を$x$とし，2層のweight layer(畳み込み層)を$f(x)$とする．入力$x$はweight layerと活性化関数ReLUを経由し，$f(x)$として出力される．
そしてスキップ接続の$x$を加算し，最終的な出力は$f(x)+x$である．
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/resnet.png}
  \caption{Residual learning\cite{he2015deep}}
  \label{fig:res_arch}
\end{figure}
\subsection{Transformer}
Transformerは，それまで機械翻訳モデルで多く使われてきた畳み込みニューラルネットワーク・再帰ニューラルネットワークのような複雑なアーキテクチャを持つネットワークとは違い
Attention機構のみを用いて構成されているエンコーダ・デコーダモデルである\cite{vaswani2017attention}．
Transformerのアーキテクチャを\fref{fig:tr_arch}に示す．
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/transformer.png}
  \caption{Transformerのアーキテクチャ\cite{vaswani2017attention}}
  \label{fig:tr_arch}
\end{figure}
\subsection{Vision Transformer}
Vision Transformerは，機械翻訳で用いられていたTransformerをコンピュータビジョンに適応させたモデルであり，画像を複数のパッチに分割してそれぞれをベクトルとして埋め込み，平坦化して入力とする特徴がある\cite{dosovitskiy2021image}．
Vision Transformerのアーキテクチャを\fref{fig:vit_arch}に示す．
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/vit.png}
  \caption{Vision Transformerのアーキテクチャ\cite{dosovitskiy2021image}}
  \label{fig:vit_arch}
\end{figure}
\section{使用ツール・実験環境}
\subsection{Python}
Pythonは1990年代の始め，オランダにあるStichting Mathematisch CentrumでGuido van RossumによってABCと呼ばれる言語の後継言語として生み出された．
Pythonはコードを簡潔に書くことができ，数値計算のNumPy，データ解析のPandasなど，専門的なライブラリが充実していることから，機械学習の研究開発をはじめとしたさまざまな分野で使用されている言語である．
簡単にPythonを始めるディストリビューションとしてAnacondaがよく使われる．また，本実験で使用したバージョンは3.7.10である．
\subsection{PyTorch}
PyTorchはディープラーニング・プロジェクトの構築を容易にする，Pythonのライブラリである．柔軟性を重視した設計であり，さらに，ディープラーニングモデルをPythonの慣用的なクラスや関数の取り扱い方
で実装できるようになっている．本実験で使用したバージョンは1.9.1である．
\subsection{Pytorch Image models}
Pytorch Image Models（timm）はRoss Wightmanによって作成されたディープラーニングライブラリであり，コンピュータビジョンの最先端のモデルが集められている．
数行の記述でモデルを呼び出すことができ，必要に応じて書き換えることで，さまざまなタスクに適用できる．本実験で使用したバージョンは0.5.2である．
\subsection{Albumentations}
AlbumentationsはPythonのためのデータ拡張のライブラリである．
モデルの汎化性能を上げるために行うデータ拡張のメソッドを多数揃えており，パイプラインを構成するとデータを効率的に拡張できる．本実験で使用したバージョンは1.1.0である．
\subsection{ImageNet}
ImageNetはディープラーニング研究のために無償で利用できる大規模なデータセットのことで，モデルの性能を測るためのベンチマークとして使われる．
本実験ではImageNetの中でも「ILSVRC-2012 ImageNet」を使用する．このデータセットは，1000のクラス，130万枚の画像で構成されている．
また，事前学習の規模によって認識精度が向上するViTの性質を確かめるために，より大規模なデータセットとして，ImageNet-21kを使用した．このデータセットは21,000のクラス及び1,400万枚の画像で構成されている．
\subsection{Kaggle}
Kaggleは世界規模のデータサイエンスのプラットフォームであり，世界中のデータサイエンティストが技術を競う場である．
本実験ではKaggleが提供しているNotebookのGPUを利用して学習及び推論を行う．
\subsection{Plant Pathology 2021 - FGVC8}
本実験ではKaggle上で提供されているデータセットのPlant Pathology2021を用いる．
このデータセットの構成を\tref{tb:dataset}に示す．
\begin{table}[htbp]
  \caption{データセットの構成}
  \label{tb:dataset}
  \centering\begin{tabular}{c|ccc}\hline
    データセットの構成 & 説明\\ \hline
    test\_images & 3枚の画像\\ \hline
    train\_images & 18,632枚の画像\\ \hline
    sample\_submission.csv & 提出用のサンプルcsvファイル\\ \hline
    train.csv & image，labelsの2カラムのcsvファイル\\ \hline
  \end{tabular}
\end{table}
\section{論文の構成}
2章では実験に使用するモデルViTのアーキテクチャを数式を用いて説明し，実験の手順や条件について示す．3章では実験結果を図と表を用いて示す．
4章では3章の結果をもとに議論を行い，理解を深める．5章では今回の実験の総括を行う．

\chapter{実験モデル}

\section{ネットワークモデル}
ViTの入力は画像である．画像サイズを\textit{H}，\textit{W}，チャネル数を\textit{C}とすると，入力\textit{x}は\eref{eq:network-input1}のように表せる．
\begin{equation}
  x\in\mathbb{R}^{H\times W\times C}
  \label{eq:network-input1}
\end{equation}
また，二次元画像を扱うために，二次元の平坦化したパッチに整形する．\eref{eq:network-input2}に示す．
\begin{equation}
  x_{p}\in\mathbb{R}^{N\times(P^{2}\times C)}
  \label{eq:network-input2}
\end{equation}
ここで，\textit{N}はパッチ数であり，\textit{P}はパッチサイズである．また，\textit{N}は\textit{H}，\textit{W}，\textit{P}を用いて\eref{eq:network-input3}のように表せる．
\begin{equation}
  N=\frac{HW}{P^{2}}
  \label{eq:network-input3}
\end{equation}
ViTのエンコーダに入力するには，$x_{p}$をさらに埋め込む必要がある．長さ$P^{2}\times C$を\textit{D}次元のベクトルとして，線形投影したものを埋め込みパッチ$z_{0}$とする．
また，入力データの先頭には[class]トークンを付与し，埋め込みパッチ$z^{0}_{0}=x_{class}とする$．\eref{eq:network-input4}に示す．
\begin{equation}
  z_{0}=[x_{class};\ x^{1}_{p}E;\ x^{2}_{p}E;\cdot\cdot\cdot;\ x^{N}_{p}E]+E_{pos},\qquad E\in\mathbb{R}^{(P^{2}\times C)\times D},\ E_{pos}\in\mathbb{R}^{(N+1)\times D}
  \label{eq:network-input4}
\end{equation}
ここで，\textit{E}はパッチを\textit{D}次元のベクトルへの埋め込みを示し，$E_{pos}$は各パッチの位置が一意に定まるように情報を付加する位置エンコーディングを表す．
上式で得られた入力$z_{0}$はMulti-Head Attention（以下，MSAと称する）に入力される．MSAでは，Attentionを求める計算を複数回行う．Attentionの算出式を\eref{eq:network-input5}に示す．
\begin{equation}
  Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V
  \label{eq:network-input5}
\end{equation}
初めに，入力ベクトルから\textit{Q，K，V}ベクトルを生成する．このとき，\textit{Q，K}の次元は$d_{k}$で，\textit{V}の次元は$d_{k}$である．
計算時は，\textit{Q，K}の要素積を$\sqrt{d_{k}}$で除算し，softmax関数で0から1の値にする．最後に\textit{V}をかけるとAttentionスコアが算出できる．
Attentionの算出過程を\fref{fig:attention}に示す．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/attention.png}
  \caption{Attentionの算出過程}
  \label{fig:attention}
\end{figure}

\section{モデルのパラメータ}
本実験で使用した各モデルのパラメータ及び設定を\tref{tb:param}に示す．
\begin{table}[htbp]
  \caption{各モデルのパラメータ及び設定}
  \label{tb:param}
  \centering\begin{tabular}{c|ccc}\hline
    \backslashbox{指標}{モデル} & vit\_base\_patch\_224 & resnet18d & vgg11\\\hline
    best valid accuracy & 0.5906 & 0.8209 & 0.8384\\\hline
    best valid f1-Score & 0.4453 & 0.6283 &0.6349\\\hline
    best valid loss & 1.1983 & 0.5554 &0.5132\\\hline
  \end{tabular}
\end{table}

\section{評価指標}
\subsection{Accuracy}
Accuracyとは
\subsection{F1-Score}
F1-Scoreとは
\subsection{Cross Entropy Loss}
Cross Entropy Lossとは

\section{手順}
実験の条件を\tref{tb:conditions}に示す．
\begin{table}[htbp]
  \caption{実験の条件}
  \label{tb:conditions}
  \centering\begin{tabular}{c|cccc}\hline
    \backslashbox{条件}{オプション} & 事前学習 & データ拡張 & ILSVRC-2012 ImageNet & ImageNet-21k\\\hline
    条件1 & なし & なし & $\circ$ & $\times$\\\hline
    条件2 & あり & なし & $\circ$ & $\times$\\\hline
    条件3 & あり & あり & $\circ$ & $\times$\\\hline
    条件4（ViTのみ） & あり & あり & $\circ$ & $\circ$\\\hline
  \end{tabular}
\end{table}

実験手順を以下に示す．

\begin{enumerate}
  \item 条件1では，事前学習・データ拡張を行わなずに認識精度を検証する．論文より\cite{dosovitskiy2021image}，ViTは大規模なデータセットで事前学習を行なった場合に良い認識精度を発揮することがわかっているので，
以降，このスコアを一つの基準として比較を進める．
  \item 条件2では，事前学習を行い，データ拡張は行わない．予想される結果として，訓練データにオーバーフィッティングすることが考えられる．
  \item 条件3では，オーバーフィッティングを防ぎ，検証データに対して認識精度を向上させるために，データ拡張を行う．これにより，訓練時と検証時の各指標の差異が小さくなると考えられる．
  また，今回の実験で使用したデータ拡張はResize，Rotate，Horizontal Flip，Grid Dropout，Brightness Contrastである．それぞれ\fref{fig:aug}に示す．
  \begin{figure}[H]
    \begin{tabular}{cc}
      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/original.png}
        \subcaption{Original}
        %\label{Accuracy}
      \end{minipage} &
      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/resize.png}
        \subcaption{Resize}
        %\label{F1-Score}
      \end{minipage} \\
  
      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/random_rotate.png}
        \subcaption{Rotate}
        %\label{Loss}
      \end{minipage} &
      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/random_brightness_contrast.png}
        \subcaption{Brightness Contrast}
        %\label{Loss}
      \end{minipage} \\

      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/holizontal_flip.png}
        \subcaption{Horizontal Flip}
        %\label{Loss}
      \end{minipage} &
      \begin{minipage}[t]{0.45\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.43]{figs/grid_dropout.png}
        \subcaption{Grid Dropout}
        %\label{Loss}
      \end{minipage}
    \end{tabular}
    \caption{条件1の実験結果}
    \label{fig:aug}
  \end{figure}
  \item 条件4では，ViTのみで実験を行う．先に述べた通り，ViTは事前学習の規模が大きいほど各画像認識のベンチマークで良い認識結果が得られている．そこで，条件3までで事前学習に使用してきたデータセットのILSVRC-2012 ImageNetに加え，
  より規模の大きいImageNet-21kを使用する．これにより各指標がこれまでよりいい結果を示すと予想される．
\end{enumerate}


\chapter{実験結果}

条件ごとの実験結果を\tref{tb:result1}，\tref{tb:result2}，\tref{tb:result3}，\tref{tb:result4}に，
また，それに対応するグラフを\fref{fig:re1}，\fref{fig:re2}，\fref{fig:re3}，\fref{fig:re4}示す．
\newpage
\begin{table}[htbp]
  \caption{条件1の実験結果}
  \label{tb:result1}
  \centering\begin{tabular}{c|ccc}\hline
    \backslashbox{指標}{モデル} & vit\_base\_patch\_224 & resnet18d & vgg11\\\hline
    best valid accuracy & 0.5906 & 0.8209 & 0.8384\\\hline
    best valid f1-Score & 0.4453 & 0.6283 &0.6349\\\hline
    best valid loss & 1.1983 & 0.5554 &0.5132\\\hline
  \end{tabular}
\end{table}

%条件1の各指標の結果を\fref{fig:re1}に示す．
\begin{figure}[H]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result1-1.png}
      \subcaption{Accuracy}
      %\label{Accuracy}
    \end{minipage} &
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result1-2.png}
      \subcaption{F1-Score}
      %\label{F1-Score}
    \end{minipage} \\

    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result1-3.png}
      \subcaption{Loss}
      %\label{Loss}
    \end{minipage}
  \end{tabular}
  \caption{条件1の実験結果}
  \label{fig:re1}
\end{figure}
\newpage
\begin{table}[htbp]
  \caption{条件2の実験結果}
  \label{tb:result2}
  \centering\begin{tabular}{c|ccc}\hline
    \backslashbox{指標}{モデル} & vit\_base\_patch\_224 & resnet18d & vgg11\\\hline
    best valid accuracy & 0.8312 & 0.8568 & 0.8611\\\hline
    best valid f1-Score & 0.6605 & 0.6708 &0.6798\\\hline
    best valid loss & 0.5562 & 0.4989 &0.4433\\\hline
  \end{tabular}
\end{table}

%条件2の各指標の結果を\fref{fig:re2}に示す．
\begin{figure}[H]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.42]{figs/result2-1.png}
      \subcaption{Accuracy}
      %\label{composite}
    \end{minipage} &
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.42]{figs/result2-2.png}
      \subcaption{F1-Score}
      %\label{Gradation}
    \end{minipage} \\

    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.42]{figs/result2-3.png}
      \subcaption{Loss}
      %\label{fill}
    \end{minipage}
  \end{tabular}
  \caption{条件2の実験結果}
  \label{fig:re2}
\end{figure}
\newpage
\begin{table}[htbp]
  \caption{条件3の実験結果}
  \label{tb:result3}
  \centering\begin{tabular}{c|ccc}\hline
    \backslashbox{指標}{モデル} & vit\_base\_patch\_224 & resnet18d & vgg11\\\hline
    best valid accuracy & 0.8806 & 0.8881 & 0.8856\\\hline
    best valid f1-Score & 0.7199 & 0.7141 &0.7114\\\hline
    best valid loss & 0.3809 & 0.3626 &0.3635\\\hline
  \end{tabular}
\end{table}
%条件3の各指標の結果を\fref{fig:re3}に示す．
\begin{figure}[H]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result3-1.png}
      \subcaption{Accuracy}
      %\label{composite}
    \end{minipage} &
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result3-2.png}
      \subcaption{F1-Score}
      %\label{Gradation}
    \end{minipage} \\

    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result3-3.png}
      \subcaption{Loss}
      %\label{fill}
    \end{minipage}
  \end{tabular}
  \caption{条件3の実験結果}
  \label{fig:re3}
\end{figure}

\newpage
\begin{table}[htbp]
  \caption{条件4の実験結果}
  \label{tb:result4}
  \centering\begin{tabular}{c|ccc}\hline
    \backslashbox{指標}{モデル} & \multicolumn{2}{c}{vit\_base\_patch\_224}\\\hline
    データセット & ILSVRC-2012 ImageNet & ImageNet-21k \\\hline
    best valid accuracy & 0.8997 & 0.9057\\\hline
    best valid f1-Score & 0.7284 & 0.7413\\\hline
    best valid loss & 0.3301 & 0.3086\\\hline
  \end{tabular}
\end{table}

%Attentionの算出過程を\fref{fig:a}に示す．
%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.9\linewidth]{figs/result1-1.png}
%  \caption{Attentionの算出過程}
%  \label{fig:a}
%\end{figure}
%条件4の各指標の結果を\fref{fig:re4}に示す．
\begin{figure}[H]
  \begin{tabular}{cc}
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result4-1.png}
      \subcaption{Accuracy}
      %\label{composite}
    \end{minipage} &
    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result4-2.png}
      \subcaption{F1-Score}
      %\label{Gradation}
    \end{minipage} \\

    \begin{minipage}[t]{0.45\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.43]{figs/result4-3.png}
      \subcaption{Loss}
      %\label{fill}
    \end{minipage}
  \end{tabular}
  \caption{条件4の実験結果}
  \label{fig:re4}
\end{figure}

\chapter{議論}
条件1の実験結果より，ViTは事前学習なしの場合では良い認識結果は得られていないことがわかる．
各指標はそれぞれ良い方向に向かっているものの，ResNet，VGGには遥かに及んでいない．これは，ViTがCNNとは違い，画像に対する帰納バイアスがないことが原因だと考えられる．
帰納バイアスとは，モデルが学習する時にもっている何らかの仮定のことである．例えばCNNであれば，「畳み込み演算のように同一カーネルによる局所的な情報処理を画像全体に渡って繰り返す2次元構造」が，
帰納バイアスとして働いていて，画像認識タスクにおいて優れた性能を発揮するに至っている．このバイアスがないことが，大規模なデータセットでの事前学習がViTに対して有効で，画像認識で良い結果を出す理由の一つである．\\
%カーネルは画像全体に同じものを適用するため，CNNは移動させたカーネルと画像のスカラ積による局所性と，画像全体に渡る同一カーネルの重みの使用による移動不変性を持つ．
%移動不変性：局所的なパターンが画像内の物体位置にかかわらず、出力に対して効果的であること．局所性：近傍における局所的な演算処理
条件2の実験結果からは，事前学習をおこなった場合のViTは他2モデルより，劣っていることがわかる．また，3モデルともAccuracy・F1-Scoreはtrainingの値のみが上がり続け，Lossは限りなく0に近づいている．
そして，validationの値は横ばいになっていることがわかる．これは，モデルが訓練データに対してオーバーフィッティングし，検証データに対する予測が上手くいっていないことが原因である．
このことから次の検証では，検証データに対して汎化性能を上げるために，入力画像を加工するデータ拡張を行った．\\
条件3の実験結果では，データ拡張をおこなった場合のViTの各指標は，training・validation共に近い値をとっており，条件2のオーバーフィッティングを解消できている．
その上，各指標の値もそれぞれ向上しており，特にViTは大きく伸びている．\\
最後にViTのみで実験を行った条件4での結果を確認する．これまでの条件と違う点は，事前学習に用いたデータセットの規模で，今までのクラス1,000，画像枚数130万枚のILSVRC-2012 ImageNetに加え，
クラス21,000，画像枚数1,400万枚のImageNet-21kを使用した点である．\fref{fig:re4}からわかるように，ViTは大規模なデータセットで学習した場合に良い認識結果を示している．
これは画像に対する帰納バイアスを持たないViTに，大量の画像による事前学習を適用したことで，ハンデを克服したと言えるだろう．しかし，ImageNet-21kで事前学習したViTは5Epochを超えたあたりから，validationとの差が広がり，オーバーフィッティングしていることもわかる．
原因は，大規模なデータセットにる事前学習の重みを適用したものの，本タスクの識別器としてはそれほど上手く機能していないことであろう．そして，この問題を解決するためには，より適切なデータ拡張やパラメータチューニングが必要だと考えられる．


\chapter{結論}
今回の実験で，Vision Transformerに事前学習を適用した時，
自分で選択したデータセットに対して良い認識結果が得られ，事前学習時のデータセットの規模が認識結果に関係していることもわかった．
また，データ拡張はモデルの過学習を防ぐのに貢献していた．従来のモデルとスコアがあまり変わらないのは，事前学習の画像枚数が論文の 1/230，1/20 で あったからだと考えられる．
そして，最近の研究では，畳み込み層とAttention層を組み合わせたモデルが，事前学習のデータセットの大小に関わらず， 良い認識結果をもたらすこともわかっている．
このことから，一般的に利用できる規模のデータセットで高い認識精度を得るには，画像認識に特化した局所性を持つ機構である畳み込み層と，
画像全体を参照しながら学習していくAttention層を適切に組み合わせる必要があると考えられる.

\backmatter% ここから後付
\chapter{謝辞}%%%%%%%%%%%%%%% 謝辞 %%%%%%%
本研究を進めるにあたり，ご指導くださった山口裕助教に感謝の意を表します．

% \begin{thebibliography}{}%%%% 参考文献 %%%
%   \bibitem{}
% \end{thebibliography}
\bibliographystyle{junsrt}%           BibTeX を使う場合
\bibliography{bibitem}% BibTeX を使う場合

\appendix% ここから付録 %%%%% 付録 %%%%%%%
\chapter{実験結果の図}
付録があればここに書く．

\end{document}
