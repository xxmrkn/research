\documentclass[a4paper, oneside, openany, dvipdfmx]{suribt}% 本文が * ページ以下のときに (掲示に注意)
\usepackage{graphicx}
\usepackage{newtxtext,newtxmath}
\usepackage{amsmath}
\usepackage{mathtools}
\title{事前学習がVision Transformerに与える影響}
%\titlewidth{}% タイトル幅 (指定するときは単位つきで)
\author{桝田 修慎}
\eauthor{Masachika Masuda}% Copyright 表示で使われる
\studentid{18A1066}
\supervisor{山口裕 助教}% 1 つ引数をとる (役職まで含めて書く)
%\supervisor{指導教員名 役職 \and 指導教員名 役職}% 複数教員の場合，\and でつなげる
\handin{2022}{2}% 提出月. 2 つ (年, 月) 引数をとる
\keywords{Vision Transformer} % 概要の下に表示される

\newcommand{\fref}[1]{図\ref{#1}}
\newcommand{\tref}[1]{表\ref{#1}}
\newcommand{\eref}[1]{式\eqref{#1}}

\begin{document}
\maketitle%%%%%%%%%%%%%%%%%%% タイトル %%%%
  事前学習がVision Transformerに与える影響

\frontmatter% ここから前文
\begin{abstract}%%%%%%%%%%%%% 概要 %%%%%%%%
  リザバー計算を用いる．
\end{abstract}

\tableofcontents%%%%%%%%%%%%% 目次 %%%%%%%%


\mainmatter% ここから本文 %%% 本文 %%%%%%%%
\chapter{序論}
\section{背景}
近年，画像認識分野では，機械翻訳で脚光を浴びることになったTransformer\cite{vaswani2017attention}をコンピュータビジョンに適応させたVision Transformer（以下ViTと称する）が登場した\cite{dosovitskiy2021image}．
ViTは，層を深くし畳み込みを行う畳み込みニューラルネットワーク（以下CNNという）とは違い，畳み込み演算をAttention機構を用いて代用しており，特に大規模なデータで事前学習を行なったときの，小・中規模の画像認識ベンチマーク（ImageNet，CIFAR-100，VTAB，etc．）では，
過去のstate-of-the-artのCNNモデルと比べて少ない計算リソースで訓練することができ、更に素晴らしい結果も出している．

本研究では，Vision Transformerが提案された論文
「An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale」を参考にし，事前学習やデータ拡張の有無が，学習及び推論に与える影響を検証した．
\section{本研究の目的}
本研究の目的を以下に示す．
\begin{enumerate}
  \item 一定の条件下での振る舞いを従来のモデル（VGG，ResNet）と比較し，ViTの優れている点・そうではない点を明らかにする．
  \item 事前学習やデータ拡張が各モデルに及ぼす影響を調べる．
\end{enumerate}

\section{深層学習}
深層学習とは，脳の神経回路を模したニューラルネットワークをより深くしたものを指し，入力データから有用な特徴量を自動で抽出する手法である．
\subsection{畳み込みニューラルネットワーク}
畳み込みニューラルネットワーク（CNN）は，入力した画像に対して重み行列（カーネル）を移動させながらスカラ積を求めていく畳み込み層を複数重ねているネットワークのことである．
カーネルは画像全体に同じものを適用するため，CNNは移動させたカーネルと画像のスカラ積による局所性と，画像全体に渡る同一カーネルの重みの使用による移動不変性を持つ．
%移動不変性：局所的なパターンが画像内の物体位置にかかわらず、出力に対して効果的であること．局所性：近傍における局所的な演算処理
\subsection{VGG\cite{simonyan2015deep}}
VGGはCNNの一種で，3x3という小さいカーネルを用いており，2014年当時では珍しい16層及び19層の深いネットワークである．
畳み込みとプーリング，線形層のシンプルなアーキテクチャであるにも関わらず，2014年のImageNetチャレンジのローカリゼーション・クラシフィケーションタスクで1位と2位を収めている．

\subsection{ResNet\cite{he2015deep}}
一般的に，CNNでは層を重ねることで，より高次元な特徴を抽出することができるが，層が深くなるにつれて勾配が発散・消失するという問題があった．
しかし，ResNetは，畳み込み層を重ねるだけではなく，前の層，もしくはより浅い層の出力を次の層の入力とする残差機構（ショートカット結合）を取り入れることで，より畳み込み層を多く積み重ねながらも，SoTAを達成した．
ResNetのブロックの一部を\fref{fig:res_arch}に示す．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/resnet.png}
  \caption{Residual learning}
  \label{fig:res_arch}
\end{figure}
\subsection{Transformer}
Transformerは，それまで機械翻訳モデルで多く使われてきた畳み込みニューラルネットワーク・再帰ニューラルネットワークのような複雑なアーキテクチャを持つネットワークとは違い
Attention機構のみを用いて構成されているエンコーダ・デコーダモデルである．
Transformerのアーキテクチャを\fref{fig:tr_arch}に示す．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/transformer.png}
  \caption{Transformerのアーキテクチャ}
  \label{fig:tr_arch}
\end{figure}
\subsection{Vision Transformer}
Vision Transformerは，機械翻訳で用いられていたTransformerをコンピュータビジョンに適応させたモデルであり，画像を複数のパッチに分割してそれぞれをベクトルとして埋め込み，平坦化して入力とする特徴がある．
Vision Transformerのアーキテクチャを\fref{fig:vit_arch}に示す．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/vit.png}
  \caption{Vision Transformerのアーキテクチャ}
  \label{fig:vit_arch}
\end{figure}
\section{論文の構成}
論文の構成を書く．こんにちはおはようごとざいますこんにちわ,本当ですか


\chapter{実験モデル}
\section{ネットワークモデル}
ネットワーク出力$z$は\eref{eq:network-output}で得られる．
ResNet

\begin{equation}
  z = W_\mathrm{out} x + b
  \label{eq:network-output}
\end{equation}

\section{手順}
実験の条件を\tref{tb:conditions}に示す．
\begin{table}[htbp]
  \caption{実験の条件}
  \label{tb:conditions}
  \centering\begin{tabular}{c|ccc}\hline
    条件 & 事前学習 & データ拡張\\\hline
    条件1 & なし & なし\\\hline
    条件2 & なし & あり\\\hline
    条件3 & あり & あり\\\hline
  \end{tabular}
\end{table}
%\begin{table}[htbp]
%  \caption{実験の条件}
%  \label{tb:requirement}
%  \centering\begin{tabular}{c|ccc}\hline
%    条件 & 事前学習 & データ拡張 & image_size & batch_size & epoch & lr & optimizer & criterion\\\hline
%    条件1 & なし & なし & 224x224 & 32 & 10 & 1e-4 & Adam & Cross Entropy Loss\\\hline
%    条件2 & なし & あり & 224x224 & 32 & 10 & 1e-4 & Adam & Cross Entropy Loss\\\hline
%    条件3 & あり & あり & 224x224 & 32 & 10 & 1e-4 & Adam & Cross Entropy Loss\\\hline
%  \end{tabular}
%\end{table}
\begin{itemize}
  \item 条件1:事前学習なし・データ拡張なし
  \item 条件2:事前学習なし・データ拡張あり
  \item 条件3:事前学習あり・データ拡張あり
\end{itemize}

実験手順を以下に示す．

\begin{enumerate}
  \item ステップ１
  \item ステップ２
\end{enumerate}


\chapter{実験結果}

実験結果を\fref{fig:pcolormesh}に示す．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/pcolormesh.eps}
  \caption{pcolormesh}
  \label{fig:pcolormesh}
\end{figure}

条件ごとの結果を\tref{tb:result}に示す．
\begin{table}[htbp]
  \caption{条件ごとの実験結果}
  \label{tb:result}
  \centering\begin{tabular}{c|ccc}\hline
    条件 & 事前学習 & データ拡張\\\hline
    条件1 & なし & なし\\\hline
    条件2 & なし & あり\\\hline
    条件3 & あり & あり\\\hline
  \end{tabular}
\end{table}

\chapter{議論}
議論を書く．

\chapter{結論}
結論を書く．

\backmatter% ここから後付
\chapter{謝辞}%%%%%%%%%%%%%%% 謝辞 %%%%%%%
謝辞を書く．

% \begin{thebibliography}{}%%%% 参考文献 %%%
%   \bibitem{}
% \end{thebibliography}
\bibliographystyle{junsrt}%           BibTeX を使う場合
\bibliography{bibitem}% BibTeX を使う場合

\appendix% ここから付録 %%%%% 付録 %%%%%%%
\chapter{実験結果の図}
付録があればここに書く．

\end{document}
